{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity of Cities\n",
    "\n",
    "<p>\n",
    "You know how different cities have a different feel when you visit them?\n",
    "Sometimes, you can find unique types of business in them. In some cities, this is caused by environment, in some by history. No matter what is behind it, the whole area is prone to accept more gladly certain types of business over the others.</p>\n",
    "<p>\n",
    "As with any data analysis, that seems like common sense, but is it true?\n",
    "I decided to check by determining the most common business categories mentioned in the Yelp data set. And comparing cities based on the top 15 business categories.</p>\n",
    "<p>\n",
    "Here is how I approached that issue.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Importing necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lexa/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nltk as nl\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is importing data from JSON database and forming data frame in the pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName = \"/Users/Lexa/Documents/coding/yelp-api/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json\"\n",
    "colNames = ['name', 'full_address', 'city', 'state', 'latitude', 'longitude', \n",
    "            'stars', 'review_count', 'categories', 'open']\n",
    "\n",
    "f = open(fileName, 'rU')\n",
    "\n",
    "data =[]\n",
    "for line in f:    \n",
    "    test = json.loads(line)\n",
    "    data.append(test)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, I like to pickle my database. Years of experience of unexpected malfunctions taught me to save often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('out.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, I'm starting with NLP procedures.\n",
    "First I need to clean up the Yelp categories. They are imputed by people without constraints, meaning that there is a mix of upper and lower cases, singular and plural, and unnecessary adjectives. </p>\n",
    "<p>\n",
    "So I'll start by making a function that will stem words in the categories.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(categoryList):\n",
    "    porter = nl.PorterStemmer()\n",
    "    cati=[]\n",
    "    tokens=[]\n",
    "    words=[]\n",
    "    vocab=[]\n",
    "    for c in categoryList:\n",
    "        tokens = nl.word_tokenize(c)\n",
    "        words = [w.lower() for w in tokens]\n",
    "        vocab = sorted(set(words))\n",
    "        vocStem=[porter.stem(t) for t in vocab]\n",
    "        if vocStem:\n",
    "            for i in range(len(vocStem)):\n",
    "                if '&' in vocStem[i]:\n",
    "                    pass\n",
    "                else:\n",
    "                    cati.append(vocStem[i])\n",
    "    return cati\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, making a list of the unique values of the cities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city=df['city']\n",
    "cities=np.unique(city.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can form dictionary where I'll gather all business categories of a town in one place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    }
   ],
   "source": [
    "cityCat={}\n",
    "\n",
    "for cit in cities:\n",
    "    if ',' in cit:\n",
    "        test=cit.split(',')\n",
    "        cit=test[0]\n",
    "    citL=cit.lower()\n",
    "    df1 = df[df['city'] == cit]\n",
    "    cate=df1['categories']\n",
    "    for ca in cate:\n",
    "        cat=cleanup(ca)\n",
    "        if citL in cityCat:\n",
    "            for k in range(len(cat)-1):\n",
    "                c=cat[k]\n",
    "                cityCat[citL].append(c)\n",
    "        else:\n",
    "            cityCat[citL] = cat    \n",
    "print len(cityCat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Of course, a mixture of the letter cases exists in CITY column as well. So I decided to put all city names as lower cases. And that reduced the number of the towns to 362. </p>\n",
    "<p>\n",
    "Some of the column values have actually the whole address in it instead of a city name. However, since I'm looking for the categories that repeat more than 2 times, this is not a problem. Such data will be removed from the analysis in the next steps.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Saving the dataframe with cleaned up categories. \n",
    "\n",
    "pkl_file = open('out_cityCat.pkl', 'wb')\n",
    "pickle.dump(cityCat,pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finding the most common categories\n",
    "\n",
    "<p>So, next step is determining which categories are most common in cities.</p> <p>\n",
    " The procedure is relatively straightforward. I'm actually just determining the frequency of certain words. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities=cityCat.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>My exploratory analysis from before showed that restaurants are quite typical in most cities, meaning that word will end up in the top 15 for sure, especially for the small places that have not so much variety. </p>\n",
    "<p>Therefore, I think it is prudent also to determine the frequency of pairs of words and triplets of the words. This approach might give me a more accurate description of the typicalities of the business in the area.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nl.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nl.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Next step is to calculate frequency of the words and phrases.</p>\n",
    "<p> Because of mistakes in the cities themselves, I'm applying frequency filter to remove words that appear less than two times. This procedure will eliminate categories from any falsely imputed names of cities, leaving those lists empty. So in the next step, I will quickly remove such cities from my analysis. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cityCatTop2={}\n",
    "cityCatTop3={}\n",
    "cityCatCommon={}\n",
    "\n",
    "for i in range(len(cities)):\n",
    "    cit=cities[i]\n",
    "    category=cityCat[cit]\n",
    "    fdist1 = nl.FreqDist(category)\n",
    "    if cit in cityCatCommon:\n",
    "        cat=fdist1.most_common(3) \n",
    "        for k in range(len(cat)-1):\n",
    "            c=cat[k]\n",
    "            cityCatCommon[cit].append(c)\n",
    "    else:\n",
    "        cityCatCommon[cit] = fdist1.most_common(3) \n",
    "    finder = nl.BigramCollocationFinder.from_words(category)\n",
    "    finder2=nl.TrigramCollocationFinder.from_words(category)\n",
    "    #finder.apply_freq_filter(2)\n",
    "    #finder2.apply_freq_filter(2)\n",
    "    if cit in cityCatTop2:\n",
    "        cat=finder.nbest(bigram_measures.pmi, 1)\n",
    "        for k in range(len(cat)-1):\n",
    "            c=cat[k]\n",
    "            cityCatTop2[cit].append(c)\n",
    "    else:\n",
    "        cityCatTop2[cit] = finder.nbest(bigram_measures.pmi, 1)\n",
    "    if cit in cityCatTop3:\n",
    "        cat=finder2.nbest(trigram_measures.pmi, 1)\n",
    "        for k in range(len(cat)-1):\n",
    "            c=cat[k]\n",
    "            cityCatTop3[cit].append(c)\n",
    "    else:\n",
    "        cityCatTop3[cit] = finder2.nbest(trigram_measures.pmi, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, I'm saving newly obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_fil = open('out_cityCatTop.pkl', 'wb')\n",
    "pickle.dump (cityCatCommon, pkl_fil)\n",
    "pkl_fil.close()\n",
    "\n",
    "pkl_file2 = open('out_cityCatBigramTop.pkl', 'wb')\n",
    "pickle.dump (cityCatTop2, pkl_file2)\n",
    "pkl_file2.close()\n",
    "\n",
    "pkl_file3 = open('out_cityCatTrigramTop.pkl', 'wb')\n",
    "pickle.dump (cityCatTop3, pkl_file3)\n",
    "pkl_file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
